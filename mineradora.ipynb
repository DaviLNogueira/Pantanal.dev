{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from threading import Thread\n",
    "from multiprocessing import cpu_count\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Essas credenciais são de um usuario meu da Amazon AWS\n",
    "##### Retirar ao enviar para o github, e colocar as suas credenciais ao minerar\n",
    "\n",
    "##### Chave de acesso e chave secreta criadas a partir de uma conta na AWS\n",
    "\n",
    "##### Pegar User agent especifico para o seu PC: https://www.whatismybrowser.com/detect/what-is-my-user-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My acess key from Aws\n",
    "ACESS_KEY = os.environ.get('ACESS_KEY')\n",
    "SECRET_KEY = os.environ.get('SECRET_KEY')\n",
    "USER_AGENT = os.environ.get('USER_AGENT')\n",
    "\n",
    "HEADERS = ({'User-Agent': USER_AGENT, 'Accept-Language': 'pt-br;q=0.5', 'x-amz-access-key': ACESS_KEY, 'x-amz-secret-key': SECRET_KEY})\n",
    "\n",
    "URL = \"https://www.amazon.com/s?k=playstation&ref=nb_sb_noss_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"a\", attrs={\"data-hook\":'product-link'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "def get_review_comments(soup):\n",
    "    array_of_comments = []\n",
    "    try:\n",
    "        review_comment = soup.find_all(\"span\", attrs={\"class\": \"cr-original-review-content\"})\n",
    "\n",
    "        for review in review_comment:\n",
    "            array_of_comments.append(review.string.strip())\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    return array_of_comments\n",
    "\n",
    "def get_reviews(soup, dictionary):\n",
    "    review_comments = get_review_comments(soup)\n",
    "\n",
    "    for review in review_comments:\n",
    "        dictionary['produto'].append(get_title(soup))\n",
    "        dictionary['avaliacoes'].append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* This function can be used later for paging, when searching for products, and only that by the moment\n",
    "def get_next_page_of_search(soup): \n",
    "    page = soup.find('ul', attrs={'class': 'a-pagination'})\n",
    "\n",
    "    if not page.find('li', attrs={'class': 'a-disabled a-last'}):\n",
    "        url = str(page.find('li', attrs={'class': 'a-last'}).find('a')['href'])\n",
    "        return url\n",
    "    \n",
    "    else:\n",
    "        return\n",
    "\n",
    "def get_next_page_of_reviews(soup):\n",
    "    next_page = soup.find(\"ul\", attrs={'class': 'a-pagination'})\n",
    "\n",
    "    if next_page is not None:\n",
    "        next_page = next_page.find(\"li\", attrs={'class': 'a-last'})\n",
    "\n",
    "    else:\n",
    "        next_page = None\n",
    "\n",
    "    return next_page\n",
    "\n",
    "def get_all_links_from_search_page(URL):\n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    "\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "    \n",
    "    return links_list\n",
    "\n",
    "def get_link_to_review_page_from_product_page(link):\n",
    "\n",
    "    webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
    "\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    link_to_review = soup.find(\"a\", attrs={'data-hook': 'see-all-reviews-link-foot'})\n",
    "\n",
    "    if link_to_review != None:\n",
    "        return link_to_review.get('href')\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_reviews_from_the_reviews_page(array_of_link_to_reviews, dictionary):\n",
    "    for link_to_review in array_of_link_to_reviews:\n",
    "        index_ref = link_to_review.find('ref') - 1\n",
    "        index_product_reviews = link_to_review.find('s') + 2\n",
    "        product_id = link_to_review[index_product_reviews: index_ref]\n",
    "        counter_page = 1\n",
    "\n",
    "        next_page = not None\n",
    "\n",
    "        while (next_page is not None) and counter_page < 10:\n",
    "\n",
    "            url = f\"https://www.amazon.com/reviews/{product_id}?pageNumber={counter_page}&pageSize=10&sortBy=recent\"\n",
    "\n",
    "            webpage = requests.get(url, params=counter_page,headers=HEADERS)\n",
    "\n",
    "            new_soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "            get_reviews(new_soup, dictionary)\n",
    "\n",
    "            next_page = get_next_page_of_reviews(new_soup)\n",
    "\n",
    "            counter_page += 1\n",
    "\n",
    "def mine_from_the_search_page():\n",
    "    \n",
    "    dictionary = {\"produto\":[], \"avaliacoes\":[]}\n",
    "    review_link_list = []\n",
    "\n",
    "    # A partir daqui Multi-thread liberado\n",
    "    links_list = get_all_links_from_search_page(URL) \n",
    "\n",
    "    for link in links_list:\n",
    "\n",
    "        review_link = get_link_to_review_page_from_product_page(link)\n",
    "        \n",
    "        if review_link is not None:\n",
    "            review_link_list.append(review_link)\n",
    "\n",
    "\n",
    "    threads = []\n",
    "    number_of_cpus = cpu_count()\n",
    "    \n",
    "    partition = len(review_link_list) // number_of_cpus\n",
    "\n",
    "    for i in range(number_of_cpus):\n",
    "\n",
    "        if i == 0:\n",
    "            threads.append(Thread(target = mine_reviews_from_the_reviews_page, args = (review_link_list[:partition], dictionary)))\n",
    "\n",
    "        elif i == number_of_cpus - 1:\n",
    "            threads.append(Thread(target = mine_reviews_from_the_reviews_page, args = (review_link_list[partition:], dictionary)))\n",
    "\n",
    "        else:\n",
    "            threads.append(Thread(target = mine_reviews_from_the_reviews_page, args = (review_link_list[partition: partition * 2], dictionary)))\n",
    "        \n",
    "        partition += partition\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    #! Debug\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dictionary_to_CSV(dictionary):\n",
    "    dataFrame = pd.DataFrame.from_dict(dictionary)\n",
    "\n",
    "    dataFrame['produto'].replace('', np.nan, inplace=True)\n",
    "    dataFrame = dataFrame.dropna(subset=['produto'])\n",
    "\n",
    "    dataFrame.to_csv(\"amazon_data.csv\", header=True, index=False)\n",
    "\n",
    "    #! Debug\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Inicializando as Threads\n",
      "Junção de Threads feita\n"
     ]
    }
   ],
   "source": [
    "d = mine_from_the_search_page()\n",
    "dataFrame = transform_dictionary_to_CSV(d)\n",
    "\n",
    "#! Debug\n",
    "\n",
    "# import time\n",
    "# from mutagen.mp3 import MP3\n",
    "# import pygame\n",
    "\n",
    "# def reproduzir_audio(caminho):\n",
    "#     audio = MP3(caminho)\n",
    "#     pygame.mixer.init()\n",
    "#     pygame.mixer.music.load(caminho)\n",
    "#     pygame.mixer.music.play()\n",
    "#     time.sleep(audio.info.length)\n",
    "#     pygame.mixer.quit()\n",
    "#     return\n",
    "\n",
    "# reproduzir_audio(\"EtapaConcluida.mp3\")\n",
    "\n",
    "#! Fim do Debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
