{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2022.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Essas credenciais s√£o de um usuario meu da Amazon AWS\n",
    "##### Retirar ao enviar para o github, e colocar as suas credenciais ao minerar\n",
    "\n",
    "##### Pegar User agent especifico para o seu PC: https://www.whatismybrowser.com/detect/what-is-my-user-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My acess key from Aws\n",
    "ACESS_KEY = \"AKIAZMB4B3QQGC5JGR7R\"\n",
    "SECRET_KEY = \"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\"\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 OPR/97.0.0.0\"\n",
    "\n",
    "HEADERS = ({'User-Agent': USER_AGENT, 'Accept-Language': 'pt-br;q=0.5', 'x-amz-access-key': ACESS_KEY, 'x-amz-secret-key': SECRET_KEY})\n",
    "\n",
    "URL = \"https://www.amazon.com/s?k=playstation&ref=nb_sb_noss_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"a\", attrs={\"data-hook\":'product-link'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "def get_review_comments(soup):\n",
    "    array_of_comments = []\n",
    "    try:\n",
    "        review_comment = soup.find_all(\"span\", attrs={\"class\": \"cr-original-review-content\"})\n",
    "\n",
    "        for review in review_comment:\n",
    "            array_of_comments.append(review.string.strip())\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    return array_of_comments\n",
    "\n",
    "def get_reviews(soup, dictionary):\n",
    "    review_comments = get_review_comments(soup)\n",
    "\n",
    "    for review in review_comments:\n",
    "        dictionary['produto'].append(get_title(soup))\n",
    "        dictionary['avaliacoes'].append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* This function can be used later for paging, when searching for products, and only that by the moment\n",
    "def get_next_page_of_search(soup): \n",
    "    page = soup.find('ul', attrs={'class': 'a-pagination'})\n",
    "\n",
    "    if not page.find('li', attrs={'class': 'a-disabled a-last'}):\n",
    "        url = str(page.find('li', attrs={'class': 'a-last'}).find('a')['href'])\n",
    "        return url\n",
    "    \n",
    "    else:\n",
    "        return\n",
    "\n",
    "def get_next_page_of_reviews(soup):\n",
    "    next_page = soup.find(\"ul\", attrs={'class': 'a-pagination'})\n",
    "\n",
    "    if next_page is not None:\n",
    "        next_page = next_page.find(\"li\", attrs={'class': 'a-last'})\n",
    "\n",
    "    else:\n",
    "        next_page = None\n",
    "\n",
    "    return next_page\n",
    "\n",
    "def get_all_links_from_search_page(URL):\n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    "\n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "    \n",
    "    return links_list\n",
    "\n",
    "def get_link_to_review_page_from_product_page(link):\n",
    "\n",
    "    webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
    "\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    link_to_review = soup.find(\"a\", attrs={'data-hook': 'see-all-reviews-link-foot'})\n",
    "\n",
    "    if link_to_review != None:\n",
    "        return link_to_review.get('href')\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_reviews_from_the_reviews_page(array_of_link_to_reviews, dictionary):\n",
    "    for link_to_review in array_of_link_to_reviews:\n",
    "        index_ref = link_to_review.find('ref') - 1\n",
    "        index_product_reviews = link_to_review.find('s') + 2\n",
    "        product_id = link_to_review[index_product_reviews: index_ref]\n",
    "        counter_page = 1\n",
    "\n",
    "        next_page = not None\n",
    "\n",
    "        while (next_page is not None) and counter_page < 10:\n",
    "\n",
    "            url = f\"https://www.amazon.com/reviews/{product_id}?pageNumber={counter_page}&pageSize=10&sortBy=recent\"\n",
    "\n",
    "            webpage = requests.get(url, params=counter_page,headers=HEADERS)\n",
    "\n",
    "            new_soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "            get_reviews(new_soup, dictionary)\n",
    "\n",
    "            next_page = get_next_page_of_reviews(new_soup)\n",
    "\n",
    "            counter_page += 1\n",
    "\n",
    "def mine_from_the_search_page():\n",
    "    \n",
    "    dictionary = {\"produto\":[], \"avaliacoes\":[]}\n",
    "    review_link_list = []\n",
    "\n",
    "    # A partir daqui Multi-thread liberado\n",
    "    links_list = get_all_links_from_search_page(URL) \n",
    "\n",
    "    for link in links_list:\n",
    "\n",
    "        review_link = get_link_to_review_page_from_product_page(link)\n",
    "        \n",
    "        if review_link is not None:\n",
    "            review_link_list.append(review_link)\n",
    "\n",
    "    mine_reviews_from_the_reviews_page(review_link_list, dictionary)\n",
    "\n",
    "    #! Debug\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dictionary_to_CSV(dictionary):\n",
    "    dataFrame = pd.DataFrame.from_dict(dictionary)\n",
    "\n",
    "    dataFrame['produto'].replace('', np.nan, inplace=True)\n",
    "    dataFrame = dataFrame.dropna(subset=['produto'])\n",
    "\n",
    "    dataFrame.to_csv(\"amazon_data.csv\", header=True, index=False)\n",
    "\n",
    "    #! Debug\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                produto  \\\n",
      "0        $50 PlayStation Store Gift Card [Digital Code]   \n",
      "1        $50 PlayStation Store Gift Card [Digital Code]   \n",
      "2        $50 PlayStation Store Gift Card [Digital Code]   \n",
      "3        $50 PlayStation Store Gift Card [Digital Code]   \n",
      "4        $50 PlayStation Store Gift Card [Digital Code]   \n",
      "...                                                 ...   \n",
      "2232     dualshock 4¬†sem fio para PlayStation 4¬†‚Äì¬†Preto   \n",
      "2233     dualshock 4¬†sem fio para PlayStation 4¬†‚Äì¬†Preto   \n",
      "2234     dualshock 4¬†sem fio para PlayStation 4¬†‚Äì¬†Preto   \n",
      "2235     dualshock 4¬†sem fio para PlayStation 4¬†‚Äì¬†Preto   \n",
      "2236  DC's Justice League: Cosmic Chaos - PlayStation 4   \n",
      "\n",
      "                                             avaliacoes  \n",
      "0                                        Fraud Purchase  \n",
      "1     This is a Fraudulent Purchase and I hope I fin...  \n",
      "2                                            Super easy  \n",
      "3     Got a gift card for Amazon over the holidays a...  \n",
      "4                                                 Great  \n",
      "...                                                 ...  \n",
      "2232  I purchased this controller and the first one ...  \n",
      "2233                           Really Nice controller !  \n",
      "2234                  Works wonderfully. Love the feel.  \n",
      "2235                                       Ol‚Äô Reliable  \n",
      "2236                      Lego-Like game (without lego)  \n",
      "\n",
      "[2237 rows x 2 columns]\n",
      "pygame 2.1.2 (SDL 2.0.18, Python 3.10.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "d = mine_from_the_search_page()\n",
    "dataFrame = transform_dictionary_to_CSV(d)\n",
    "\n",
    "#! Debug\n",
    "\n",
    "import time\n",
    "from mutagen.mp3 import MP3\n",
    "import pygame\n",
    "\n",
    "def reproduzir_audio(caminho):\n",
    "    audio = MP3(caminho)\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(caminho)\n",
    "    pygame.mixer.music.play()\n",
    "    time.sleep(audio.info.length)\n",
    "    pygame.mixer.quit()\n",
    "    return\n",
    "\n",
    "reproduzir_audio(\"EtapaConcluida.mp3\")\n",
    "\n",
    "#! Fim do Debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
